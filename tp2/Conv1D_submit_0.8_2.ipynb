{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/joaquin/anaconda3/lib/python3.7/site-packages (3.0.2)\n",
      "Requirement already satisfied: requests in /home/joaquin/anaconda3/lib/python3.7/site-packages (from transformers) (2.22.0)\n",
      "Requirement already satisfied: packaging in /home/joaquin/anaconda3/lib/python3.7/site-packages (from transformers) (20.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/joaquin/anaconda3/lib/python3.7/site-packages (from transformers) (4.42.1)\n",
      "Requirement already satisfied: filelock in /home/joaquin/anaconda3/lib/python3.7/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92 in /home/joaquin/anaconda3/lib/python3.7/site-packages (from transformers) (0.1.91)\n",
      "Requirement already satisfied: numpy in /home/joaquin/anaconda3/lib/python3.7/site-packages (from transformers) (1.18.1)\n",
      "Requirement already satisfied: tokenizers==0.8.1.rc1 in /home/joaquin/anaconda3/lib/python3.7/site-packages (from transformers) (0.8.1rc1)\n",
      "Requirement already satisfied: sacremoses in /home/joaquin/anaconda3/lib/python3.7/site-packages (from transformers) (0.0.43)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/joaquin/anaconda3/lib/python3.7/site-packages (from transformers) (2020.7.14)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/joaquin/anaconda3/lib/python3.7/site-packages (from requests->transformers) (1.25.8)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /home/joaquin/anaconda3/lib/python3.7/site-packages (from requests->transformers) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/joaquin/anaconda3/lib/python3.7/site-packages (from requests->transformers) (2019.11.28)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/joaquin/anaconda3/lib/python3.7/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: six in /home/joaquin/anaconda3/lib/python3.7/site-packages (from packaging->transformers) (1.14.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/joaquin/anaconda3/lib/python3.7/site-packages (from packaging->transformers) (2.4.6)\n",
      "Requirement already satisfied: joblib in /home/joaquin/anaconda3/lib/python3.7/site-packages (from sacremoses->transformers) (0.14.1)\n",
      "Requirement already satisfied: click in /home/joaquin/anaconda3/lib/python3.7/site-packages (from sacremoses->transformers) (7.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "#import plotly.express as px\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train.csv')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('test.csv')\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#funciones auxiliares\n",
    "import re\n",
    "regex = re.compile('[^a-zA-Z]')\n",
    "def getPunctuationSigns(words):\n",
    "    return ' '.join([item for item in words if len(regex.sub('', item)) == 0])\n",
    "\n",
    "def removePunctuationSigns(words):\n",
    "    return [item for item in words if len(regex.sub('', item)) > 0]\n",
    "\n",
    "\n",
    "def getHashtags(words):\n",
    "    return ' '.join([item.lstrip('#') for item in words if item.startswith('#') and len(item) > 1])\n",
    "\n",
    "def getMentions(words):\n",
    "    return ' '.join([item.lstrip('@') for item in words if item.startswith('@') and len(item) > 1])\n",
    "\n",
    "def getURLs(words):\n",
    "    return ' '.join([item for item in words if item.startswith('http')])\n",
    "\n",
    "def clean(text):\n",
    "    \n",
    "    exclude = set(string.punctuation)\n",
    "    clean_text = ''.join(ch for ch in text if ch not in exclude)\n",
    "    \n",
    "    exclude = set(stopwords.words('english'))\n",
    "    clean_text_list = clean_text.split(' ')\n",
    "    clean_text = ' '.join(ch for ch in clean_text_list if ch not in exclude)\n",
    "\n",
    "    return clean_text\n",
    "\n",
    "#test_df['clean_text'] = test_df['text'].apply(clean)\n",
    "#train_df['clean_text'] = train_df['text'].apply(clean)\n",
    "#train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "\n",
    "#Si tira error porque no lo reconoce ejecutar el siguiente codigo\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "#Si tira error por no encontrar el 'averaged_perceptron_tagger' ejecutar el siguiente codigo\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def getWordNetPOSTag(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "def lemmatizer(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmaWords = []\n",
    "    for w in text.split(\" \"):\n",
    "        #lemmaWords.append(lemmatizer.lemmatize(w, getWordNetPOSTag(w)))\n",
    "        lemmaWords.append(lemmatizer.lemmatize(w, pos='a')) \n",
    "        \n",
    "    return \" \".join(lemmaWords)\n",
    "\n",
    "def porterStemmer(text):\n",
    "    porter = PorterStemmer()\n",
    "    stemmedWords = []\n",
    "    for w in text.split(\" \"):\n",
    "        stemmedWords.append(porter.stem(w)) \n",
    "        \n",
    "    return \" \".join(stemmedWords)\n",
    "\n",
    "def snowballStemmer(text):\n",
    "    porter = SnowballStemmer(\"english\")\n",
    "    stemmedWords = []\n",
    "    for w in text.split(\" \"):\n",
    "        stemmedWords.append(porter.stem(w)) \n",
    "        \n",
    "    return \" \".join(stemmedWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FEATURES\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "def crear_features(df):\n",
    "    \n",
    "    #texto\n",
    "    df['clean_text'] = df['text'].apply(clean)\n",
    "    df['lemma_text'] = df['clean_text'].apply(lemmatizer)\n",
    "    df['porter_stemmed_text'] = df['clean_text'].apply(porterStemmer)\n",
    "    df['snowball_stemmed_text'] = df['clean_text'].apply(snowballStemmer)\n",
    "    \n",
    "    #categóricas\n",
    "    df['words'] = df['text'].apply(lambda x: x.split(' '))\n",
    "    df['hashtags'] = df['words'].apply(getHashtags) #Obtengo los hashtags: \"ht1 ht2 ht3 ...\"\n",
    "    df['mentions'] = df['words'].apply(getMentions) #Obtengo las menciones: \"men1 men2 men3 ...\"\n",
    "    df['urls'] = df['words'].apply(getURLs) #Obtengo las urls \"url1 url2 url3 ...\"\n",
    "    df['stop_words'] = df['text'].apply(lambda x: [w for w in str(x).lower().split() if w in stop])\n",
    "    df['real_words'] = df['words'].apply(removePunctuationSigns)\n",
    "    \n",
    "    #numéricas\n",
    "    df['words_count'] = df['words'].apply(lambda x: len(x))\n",
    "    df['character_count'] = df['text'].str.len()\n",
    "    df['mean_word_length'] = df['text'].apply(lambda x: (sum(len(w) for w in str(x).split()) / len(str(x).split())))\n",
    "    df['punctuation_count'] = df['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n",
    "    df['stop_words_count'] = df['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in stop]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crear_features(train_df)\n",
    "crear_features(test_df)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trp(l, n):\n",
    "    return l[:n] + [0]*(n-len(l))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizacion_de_texto(texto,longitud):\n",
    "    marked_text = \"[CLS] \" + texto + \" [SEP]\"\n",
    "    tokenized_text = tokenizer.tokenize(marked_text)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    vector = trp(indexed_tokens,longitud)\n",
    "    return vector\n",
    "    #return indexed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.fillna('')\n",
    "test_df = test_df.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parametros\n",
    "vocab_size = 30522\n",
    "epochs = 3\n",
    "maxlen = 140\n",
    "n_words = 500\n",
    "test_size = 0.25\n",
    "padding_texto = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenización\n",
    "def tokenizar_df(df):\n",
    "\n",
    "    df['texto_tokenizado'] = df['text'].apply(lambda x: tokenizacion_de_texto(x,padding_texto))\n",
    "    df['lemma_text_tokenizado'] = df['lemma_text'].apply(lambda x: tokenizacion_de_texto(x,padding_texto))\n",
    "    df['porter_stemmed_text_tokenizado'] = df['porter_stemmed_text'].apply(lambda x: tokenizacion_de_texto(x,padding_texto))\n",
    "    df['snowball_stemmed_text_tokenizado'] = df['snowball_stemmed_text'].apply(lambda x: tokenizacion_de_texto(x,padding_texto))\n",
    "    df['clean_text_tokenizado'] = df['clean_text'].apply(lambda x: tokenizacion_de_texto(x,padding_texto))\n",
    "    df['keyword_tokenizado'] = df['keyword'].apply(lambda x: tokenizacion_de_texto(x,5))\n",
    "    df['location_tokenizado'] = df['location'].apply(lambda x: tokenizacion_de_texto(x,7))\n",
    "    df['mentions_tokenizado'] = df['mentions'].apply(lambda x: tokenizacion_de_texto(x,8))\n",
    "    df['hashtags_tokenizado'] = df['hashtags'].apply(lambda x: tokenizacion_de_texto(x,8))\n",
    "    df['real_words_tokenizado'] = df['real_words'].apply(lambda x: tokenizacion_de_texto(' '.join(x),8))\n",
    "\n",
    "    df['words_count'] = df['words_count'].apply(lambda x: [x])\n",
    "    df['character_count'] = df['character_count'].apply(lambda x: [x])\n",
    "    df['mean_word_length'] = df['mean_word_length'].apply(lambda x: [x])\n",
    "    df['punctuation_count'] = df['punctuation_count'].apply(lambda x: [x])\n",
    "    df['stop_words_count'] = df['stop_words_count'].apply(lambda x: [x])\n",
    "    \n",
    "    df['features_tokenizados'] = (  #TEXTO\n",
    "                                    df['texto_tokenizado']\n",
    "                                    #df['clean_text_tokenizado']\n",
    "                                    #df['lemma_text_tokenizado']\n",
    "                                    #df['porter_stemmed_text_tokenizado']\n",
    "                                    #df['snowball_stemmed_text_tokenizado']\n",
    "                                    +df['real_words_tokenizado']\n",
    "        \n",
    "                                    #CATEGORICAS\n",
    "                                    #+ df['keyword_tokenizado']\n",
    "                                    #+ df['location_tokenizado']\n",
    "                                    + df['mentions_tokenizado']\n",
    "                                    + df['hashtags_tokenizado']\n",
    "                                  \n",
    "                                    #NUMERICAS\n",
    "                                    #+ df['mean_word_length']\n",
    "                                    #+ df['stop_words_count']\n",
    "                                    #+ df['punctuation_count']\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizar_df(train_df)\n",
    "tokenizar_df(test_df)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split\n",
    "FX, FY = train_df['features_tokenizados'], train_df['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_df['features_tokenizados'], train_df['target'], test_size=test_size,\n",
    "                                                  random_state=42)\n",
    "FTest = test_df['features_tokenizados']\n",
    "test_ids = test_df['id'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array([np.array(lista) for lista in X_train])\n",
    "X_a = np.array([np.array(lista) for lista in X_test])\n",
    "\n",
    "padded_FX =   np.array([np.array(lista) for lista in FX])\n",
    "padded_FY =   np.array([np.array(lista) for lista in FY])\n",
    "\n",
    "padded_FTest = X_test = np.array([np.array(lista) for lista in FTest])\n",
    "padded_train = pad_sequences(X_train, maxlen = maxlen, truncating = 'post')\n",
    "padded_test = pad_sequences(X_a, maxlen = maxlen, truncating = 'post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#secuencia_train = tokenizar(X_train.values)\n",
    "#secuencia_test = tokenizar(X_test.values)\n",
    "#secuencia_FTest = tokenizar(FTest.values)\n",
    "\n",
    "#padded_train = pad_sequences(secuencia_train, maxlen = maxlen, truncating = 'post')\n",
    "#padded_test = pad_sequences(secuencia_test, maxlen = maxlen)\n",
    "#padded_FTest = pad_sequences(secuencia_FTest, maxlen = maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#modelo Conv1D\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, 128, input_length= maxlen),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Conv1D(256, 3, activation='relu'),\n",
    "    tf.keras.layers.GlobalMaxPooling1D(),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.fit(padded_FX, padded_FY, epochs=2)\n",
    "model.fit(padded_train,y_train,epochs=epochs, validation_data=(padded_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#validación del f1 score con el set de test\n",
    "test_val = pd.read_csv('submission.csv')\n",
    "#test_val.head()\n",
    "y_FTest = test_val['target']\n",
    "preds = model.predict_classes(padded_FTest)\n",
    "print(\"F1 score:\", f1_score(y_FTest, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_locales = model.predict_classes(padded_test)\n",
    "preds_locales = pd.Series(list((x[0] for x in preds_locales)))\n",
    "#preds_locales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"F1 score:\", f1_score(y_test, preds_locales))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preds = model.predict_classes(padded_FTest)\n",
    "#preds = pd.Series(preds)\n",
    "preds = pd.Series(list((x[0] for x in preds)))\n",
    "preds\n",
    "df_preds = pd.concat([test_ids,preds],axis=1)\n",
    "df_preds.rename(columns = {0 : 'target'}, inplace=True)\n",
    "df_preds.set_index('id', inplace=True)\n",
    "df_preds.to_csv('BERT-Conv1D-features.csv')\n",
    "df_preds.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
