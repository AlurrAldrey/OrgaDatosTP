{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('clean_train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "train['source'] = 'train'\n",
    "test['source'] = 'test'\n",
    "merged = pd.concat([train, test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTrainSet():\n",
    "    return merged.loc[merged['source'] == 'train']\n",
    "\n",
    "def getTestSet():\n",
    "    return merged.loc[merged['source'] == 'test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agregando caracteristicas del texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getHashtags(words):\n",
    "    return ' '.join([item.lstrip('#') for item in words if item.startswith('#') and len(item) > 1])\n",
    "\n",
    "def getMentions(words):\n",
    "    return ' '.join([item.lstrip('@') for item in words if item.startswith('@') and len(item) > 1])\n",
    "\n",
    "def getURLs(words):\n",
    "    return ' '.join([item for item in words if item.startswith('http')])\n",
    "\n",
    "#import nltk\n",
    "#nltk.download('stopwords')\n",
    "#Elimina las stopwords, urls, hashtags y menciones\n",
    "def removeSpecialWords(words):\n",
    "    stop = stopwords.words('english')\n",
    "    return ' '.join([item.lstrip('#').lstrip('@') for item in words if item not in stop and not item.startswith('#') and not item.startswith('@') and not item.startswith('http')])\n",
    "\n",
    "import re\n",
    "regex = re.compile('[^a-zA-Z]')\n",
    "def getPunctuationSigns(words):\n",
    "    return ' '.join([item for item in words if len(regex.sub('', item)) == 0])\n",
    "\n",
    "def removePunctuationSigns(words):\n",
    "    return [item for item in words if len(regex.sub('', item)) > 0]\n",
    "\n",
    "def arrayToLower(arr):\n",
    "    return [item.lower() for item in arr]\n",
    "\n",
    "def getWordsLengthAVG(text):\n",
    "    words = text.split()\n",
    "    \n",
    "    if len(words) == 0:\n",
    "        return 0\n",
    "    \n",
    "    total = 0\n",
    "    for w in words:\n",
    "        total += len(w)\n",
    "        \n",
    "    return total/len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_tokenizer = TweetTokenizer()\n",
    "\n",
    "merged['words'] = merged['text'].apply(tweet_tokenizer.tokenize).apply(arrayToLower) #Tokenizacion\n",
    "merged['real_words'] = merged['words'].apply(removePunctuationSigns) #Elimino los signos de puntuacion\n",
    "merged['clean_text'] = merged['real_words'].apply(removeSpecialWords) #Elimino las stopwords, hashtags, menciones y urls\n",
    "\n",
    "merged['punctuation_signs'] = merged['words'].apply(getPunctuationSigns) #Obtengo los signos de puntuacion \"s1 s2 s2 ...\"\n",
    "merged['hashtags'] = merged['words'].apply(getHashtags) #Obtengo los hashtags: \"ht1 ht2 ht3 ...\"\n",
    "merged['mentions'] = merged['words'].apply(getMentions) #Obtengo las menciones: \"men1 men2 men3 ...\"\n",
    "merged['urls'] = merged['words'].apply(getURLs) #Obtengo las urls \"url1 url2 url3 ...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged['entities_count'] = merged['words'].apply(lambda x: len(x)) #Conteo de todas las cosas que hay (incluye TODO)\n",
    "merged['words_count'] = merged['clean_text'].apply(lambda x: len(x.split())) #Conteo de palabras\n",
    "merged['punctuations_signs_count'] = merged['punctuation_signs'].apply(lambda x: len(x.split())) #Conteo de puntuaciones\n",
    "merged['hashtags_count'] = merged['hashtags'].apply(lambda x: len(x.split())) #Conteo de hashtags\n",
    "merged['mentions_count'] = merged['mentions'].apply(lambda x: len(x.split())) #Conteo de menciones\n",
    "merged['urls_count'] = merged['urls'].apply(lambda x: len(x.split())) #Conteo de urls\n",
    "\n",
    "#Conteo de stopwords\n",
    "merged['stopwords_count'] = merged.entities_count - merged.words_count - merged.punctuations_signs_count - merged.hashtags_count - merged.mentions_count - merged.urls_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged['words_length_avg'] = merged['clean_text'].apply(getWordsLengthAVG)\n",
    "\n",
    "merged['punctuations_ratio'] = merged.punctuations_signs_count/merged.entities_count\n",
    "merged['hashtags_ratio'] = merged.hashtags_count/merged.entities_count\n",
    "merged['mentions_ratio'] = merged.mentions_count/merged.entities_count\n",
    "merged['urls_ratio'] = merged.urls_count/merged.entities_count\n",
    "merged['stopwords_ratio'] = merged.stopwords_count/merged.entities_count\n",
    "\n",
    "merged['special_entities_ratio'] = (merged.entities_count - merged.words_count)/merged.entities_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aplicando mean encoding a keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Completando valores vacios\n",
    "merged['keyword'].fillna('undefined', inplace=True)\n",
    "kw_group = merged.groupby('keyword')['target'].mean().reset_index()\n",
    "kw_group.columns = ['keyword', 'keyword_cv_mean_enc']\n",
    "\n",
    "#Agrego la columna keyword_cv_mean_enc\n",
    "merged = pd.merge(merged, kw_group, how='left', on='keyword')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 10766 entries, 0 to 10765\n",
      "Data columns (total 32 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   id                        10766 non-null  int64  \n",
      " 1   keyword                   10766 non-null  object \n",
      " 2   location                  7186 non-null   object \n",
      " 3   text                      10766 non-null  object \n",
      " 4   target                    7503 non-null   float64\n",
      " 5   country                   7503 non-null   object \n",
      " 6   city                      7503 non-null   object \n",
      " 7   lat                       4273 non-null   float64\n",
      " 8   lon                       4273 non-null   float64\n",
      " 9   source                    10766 non-null  object \n",
      " 10  words                     10766 non-null  object \n",
      " 11  real_words                10766 non-null  object \n",
      " 12  clean_text                10766 non-null  object \n",
      " 13  punctuation_signs         10766 non-null  object \n",
      " 14  hashtags                  10766 non-null  object \n",
      " 15  mentions                  10766 non-null  object \n",
      " 16  urls                      10766 non-null  object \n",
      " 17  entities_count            10766 non-null  int64  \n",
      " 18  words_count               10766 non-null  int64  \n",
      " 19  punctuations_signs_count  10766 non-null  int64  \n",
      " 20  hashtags_count            10766 non-null  int64  \n",
      " 21  mentions_count            10766 non-null  int64  \n",
      " 22  urls_count                10766 non-null  int64  \n",
      " 23  stopwords_count           10766 non-null  int64  \n",
      " 24  words_length_avg          10766 non-null  float64\n",
      " 25  punctuations_ratio        10766 non-null  float64\n",
      " 26  hashtags_ratio            10766 non-null  float64\n",
      " 27  mentions_ratio            10766 non-null  float64\n",
      " 28  urls_ratio                10766 non-null  float64\n",
      " 29  stopwords_ratio           10766 non-null  float64\n",
      " 30  special_entities_ratio    10766 non-null  float64\n",
      " 31  keyword_cv_mean_enc       10766 non-null  float64\n",
      "dtypes: float64(11), int64(8), object(13)\n",
      "memory usage: 2.7+ MB\n"
     ]
    }
   ],
   "source": [
    "merged.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>country</th>\n",
       "      <th>city</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>source</th>\n",
       "      <th>...</th>\n",
       "      <th>urls_count</th>\n",
       "      <th>stopwords_count</th>\n",
       "      <th>words_length_avg</th>\n",
       "      <th>punctuations_ratio</th>\n",
       "      <th>hashtags_ratio</th>\n",
       "      <th>mentions_ratio</th>\n",
       "      <th>urls_ratio</th>\n",
       "      <th>stopwords_ratio</th>\n",
       "      <th>special_entities_ratio</th>\n",
       "      <th>keyword_cv_mean_enc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>undefined</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>undefined</td>\n",
       "      <td>undefined</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>train</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.660714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>undefined</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1.0</td>\n",
       "      <td>undefined</td>\n",
       "      <td>undefined</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>train</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.428571</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.660714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>undefined</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>undefined</td>\n",
       "      <td>undefined</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>train</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>7.090909</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.440000</td>\n",
       "      <td>0.560000</td>\n",
       "      <td>0.660714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>undefined</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>undefined</td>\n",
       "      <td>undefined</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>train</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7.800000</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.660714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>undefined</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>undefined</td>\n",
       "      <td>undefined</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>train</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>4.571429</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.660714</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id    keyword location                                               text  \\\n",
       "0   1  undefined      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4  undefined      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5  undefined      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6  undefined      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7  undefined      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target    country       city  lat  lon source  ... urls_count  \\\n",
       "0     1.0  undefined  undefined  NaN  NaN  train  ...          0   \n",
       "1     1.0  undefined  undefined  NaN  NaN  train  ...          0   \n",
       "2     1.0  undefined  undefined  NaN  NaN  train  ...          0   \n",
       "3     1.0  undefined  undefined  NaN  NaN  train  ...          0   \n",
       "4     1.0  undefined  undefined  NaN  NaN  train  ...          0   \n",
       "\n",
       "  stopwords_count words_length_avg punctuations_ratio hashtags_ratio  \\\n",
       "0               6         4.666667              0.000       0.076923   \n",
       "1               0         4.428571              0.125       0.000000   \n",
       "2              11         7.090909              0.120       0.000000   \n",
       "3               1         7.800000              0.125       0.125000   \n",
       "4               7         4.571429              0.000       0.125000   \n",
       "\n",
       "  mentions_ratio urls_ratio  stopwords_ratio  special_entities_ratio  \\\n",
       "0            0.0        0.0         0.461538                0.538462   \n",
       "1            0.0        0.0         0.000000                0.125000   \n",
       "2            0.0        0.0         0.440000                0.560000   \n",
       "3            0.0        0.0         0.125000                0.375000   \n",
       "4            0.0        0.0         0.437500                0.562500   \n",
       "\n",
       "   keyword_cv_mean_enc  \n",
       "0             0.660714  \n",
       "1             0.660714  \n",
       "2             0.660714  \n",
       "3             0.660714  \n",
       "4             0.660714  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Si tira error porque no lo reconoce ejecutar el siguiente codigo\n",
    "#import nltk\n",
    "#nltk.download('wordnet')\n",
    "\n",
    "#Si tira error por no encontrar el 'averaged_perceptron_tagger' ejecutar el siguiente codigo\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def getWordNetPOSTag(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "def lemmatizer(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmaWords = []\n",
    "    for w in text.split(\" \"):\n",
    "        #lemmaWords.append(lemmatizer.lemmatize(w, getWordNetPOSTag(w)))\n",
    "        lemmaWords.append(lemmatizer.lemmatize(w, pos='a')) \n",
    "        \n",
    "    return \" \".join(lemmaWords)\n",
    "\n",
    "def porterStemmer(text):\n",
    "    porter = PorterStemmer()\n",
    "    stemmedWords = []\n",
    "    for w in text.split(\" \"):\n",
    "        stemmedWords.append(porter.stem(w)) \n",
    "        \n",
    "    return \" \".join(stemmedWords)\n",
    "\n",
    "def snowballStemmer(text):\n",
    "    porter = SnowballStemmer(\"english\")\n",
    "    stemmedWords = []\n",
    "    for w in text.split(\" \"):\n",
    "        stemmedWords.append(porter.stem(w)) \n",
    "        \n",
    "    return \" \".join(stemmedWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged['lemma_text'] = merged['clean_text'].apply(lemmatizer)\n",
    "merged['porter_stemmed_text'] = merged['clean_text'].apply(porterStemmer)\n",
    "merged['snowball_stemmed_text'] = merged['clean_text'].apply(snowballStemmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.to_csv('final_train.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
       "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
       "                sublinear_tf=True, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, use_idf=True, vocabulary=None)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma_vector = TfidfVectorizer(sublinear_tf=True)\n",
    "lemma_vector.fit(np.array(merged['lemma_text']).ravel())\n",
    "#lemma_vector.transform(np.array(merged['lemma_text']).ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<7503x13137 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 60588 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma_vector = TfidfVectorizer(sublinear_tf=True)\n",
    "lemma_vector.fit(np.array(getTrainSet()['clean_text']).ravel())\n",
    "lemma_vector.transform(np.array(getTrainSet()['clean_text']).ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def testColumn(columnName):\n",
    "    feature_vector = TfidfVectorizer(sublinear_tf=True)\n",
    "    feature_vector.fit(np.array(getTrainSet()[columnName]).ravel())\n",
    "\n",
    "    tf_vector = feature_vector\n",
    "    X = lemma_vector.transform(np.array(getTrainSet()[columnName]).ravel())\n",
    "    y = np.array(train['target']).ravel()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=30)\n",
    "\n",
    "    # Training Naive Bayes model\n",
    "    NB_model = MultinomialNB()\n",
    "    NB_model.fit(X_train, y_train)\n",
    "    y_predict_nb = NB_model.predict(X_test)\n",
    "    print(f1_score(y_test, y_predict_nb))\n",
    "\n",
    "    # Training Logistics Regression model\n",
    "    LR_model = LogisticRegression(solver='lbfgs')\n",
    "    LR_model.fit(X_train, y_train)\n",
    "    y_predict_lr = LR_model.predict(X_test)\n",
    "    print(f1_score(y_test, y_predict_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.741652021089631\n",
      "0.7383512544802868\n"
     ]
    }
   ],
   "source": [
    "testColumn('lemma_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7421052631578948\n",
      "0.7394807520143241\n"
     ]
    }
   ],
   "source": [
    "testColumn('clean_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7070524412296565\n",
      "0.7314487632508834\n"
     ]
    }
   ],
   "source": [
    "testColumn('porter_stemmed_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testColumn('snowball_stemmed_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "X = getTrainSet()[['entities_count', 'words_count', 'punctuations_signs_count', 'hashtags_count', 'mentions_count', 'urls_count', \n",
    "           'stopwords_count', 'words_length_avg', 'punctuations_ratio', 'mentions_ratio', 'urls_ratio', 'stopwords_ratio',\n",
    "           'special_entities_ratio', 'keyword_cv_mean_enc']].values\n",
    "Y = getTrainSet()['target'].values\n",
    "# feature extraction\n",
    "test = SelectKBest(score_func=f_classif, k=4)\n",
    "fit = test.fit(X, Y)\n",
    "# summarize scores\n",
    "set_printoptions(precision=3)\n",
    "print(fit.scores_)\n",
    "features = fit.transform(X)\n",
    "# summarize selected features\n",
    "print(features[0:5,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "X = getTrainSet()[['entities_count', 'words_count', 'punctuations_signs_count', 'hashtags_count', 'mentions_count', 'urls_count', \n",
    "           'stopwords_count', 'words_length_avg', 'punctuations_ratio', 'mentions_ratio', 'urls_ratio', 'stopwords_ratio',\n",
    "           'special_entities_ratio', 'keyword_cv_mean_enc']].values\n",
    "Y = getTrainSet()['target'].values\n",
    "# feature extraction\n",
    "model = ExtraTreesClassifier(n_estimators=10)\n",
    "model.fit(X, Y)\n",
    "print(model.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_vector = TfidfVectorizer(sublinear_tf=True)\n",
    "feature_vector.fit(np.array(getTrainSet()['lemma_text']).ravel())\n",
    "\n",
    "tf_vector = feature_vector\n",
    "X = lemma_vector.transform(np.array(getTrainSet()['lemma_text']).ravel())\n",
    "y = np.array(train['target']).ravel()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=30)\n",
    "\n",
    "X_to_predict = lemma_vector.transform(np.array(getTestSet()['lemma_text']).ravel())\n",
    "\n",
    "NB_model = MultinomialNB()\n",
    "NB_model.fit(X_train, y_train)\n",
    "\n",
    "predictions = NB_model.predict(X_to_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateSubmission(predicts, model):\n",
    "    submission = test[['id', 'target']]\n",
    "    submission['target'] = predicts\n",
    "    submission.to_csv(model + '_pred.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "generateSubmission(predictions, 'NaiveBayes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
