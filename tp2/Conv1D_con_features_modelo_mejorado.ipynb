{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/joaquin/anaconda3/lib/python3.7/site-packages (3.0.2)\n",
      "Requirement already satisfied: tokenizers==0.8.1.rc1 in /home/joaquin/anaconda3/lib/python3.7/site-packages (from transformers) (0.8.1rc1)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92 in /home/joaquin/anaconda3/lib/python3.7/site-packages (from transformers) (0.1.91)\n",
      "Requirement already satisfied: sacremoses in /home/joaquin/anaconda3/lib/python3.7/site-packages (from transformers) (0.0.43)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/joaquin/anaconda3/lib/python3.7/site-packages (from transformers) (4.42.1)\n",
      "Requirement already satisfied: packaging in /home/joaquin/anaconda3/lib/python3.7/site-packages (from transformers) (20.1)\n",
      "Requirement already satisfied: filelock in /home/joaquin/anaconda3/lib/python3.7/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/joaquin/anaconda3/lib/python3.7/site-packages (from transformers) (2020.7.14)\n",
      "Requirement already satisfied: requests in /home/joaquin/anaconda3/lib/python3.7/site-packages (from transformers) (2.22.0)\n",
      "Requirement already satisfied: numpy in /home/joaquin/anaconda3/lib/python3.7/site-packages (from transformers) (1.18.1)\n",
      "Requirement already satisfied: click in /home/joaquin/anaconda3/lib/python3.7/site-packages (from sacremoses->transformers) (7.0)\n",
      "Requirement already satisfied: joblib in /home/joaquin/anaconda3/lib/python3.7/site-packages (from sacremoses->transformers) (0.14.1)\n",
      "Requirement already satisfied: six in /home/joaquin/anaconda3/lib/python3.7/site-packages (from sacremoses->transformers) (1.14.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/joaquin/anaconda3/lib/python3.7/site-packages (from packaging->transformers) (2.4.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/joaquin/anaconda3/lib/python3.7/site-packages (from requests->transformers) (2019.11.28)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/joaquin/anaconda3/lib/python3.7/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /home/joaquin/anaconda3/lib/python3.7/site-packages (from requests->transformers) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/joaquin/anaconda3/lib/python3.7/site-packages (from requests->transformers) (1.25.8)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "#import plotly.express as px\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('train.csv')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7613 entries, 0 to 7612\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        7613 non-null   int64 \n",
      " 1   keyword   7552 non-null   object\n",
      " 2   location  5080 non-null   object\n",
      " 3   text      7613 non-null   object\n",
      " 4   target    7613 non-null   int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 297.5+ KB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text\n",
       "0   0     NaN      NaN                 Just happened a terrible car crash\n",
       "1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
       "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n",
       "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
       "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv('test.csv')\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3263 entries, 0 to 3262\n",
      "Data columns (total 4 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        3263 non-null   int64 \n",
      " 1   keyword   3237 non-null   object\n",
      " 2   location  2158 non-null   object\n",
      " 3   text      3263 non-null   object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 102.1+ KB\n"
     ]
    }
   ],
   "source": [
    "test_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#funciones auxiliares\n",
    "\n",
    "def getHashtags(words):\n",
    "    return ' '.join([item.lstrip('#') for item in words if item.startswith('#') and len(item) > 1])\n",
    "\n",
    "def getMentions(words):\n",
    "    return ' '.join([item.lstrip('@') for item in words if item.startswith('@') and len(item) > 1])\n",
    "\n",
    "def getURLs(words):\n",
    "    return ' '.join([item for item in words if item.startswith('http')])\n",
    "\n",
    "def clean(text):\n",
    "    \n",
    "    exclude = set(string.punctuation)\n",
    "    clean_text = ''.join(ch for ch in text if ch not in exclude)\n",
    "    \n",
    "    exclude = set(stopwords.words('english'))\n",
    "    clean_text_list = clean_text.split(' ')\n",
    "    clean_text = ' '.join(ch for ch in clean_text_list if ch not in exclude)\n",
    "\n",
    "    return clean_text\n",
    "\n",
    "#test_df['clean_text'] = test_df['text'].apply(clean)\n",
    "#train_df['clean_text'] = train_df['text'].apply(clean)\n",
    "#train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/nicolas/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "\n",
    "#Si tira error porque no lo reconoce ejecutar el siguiente codigo\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "#Si tira error por no encontrar el 'averaged_perceptron_tagger' ejecutar el siguiente codigo\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def getWordNetPOSTag(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "def lemmatizer(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmaWords = []\n",
    "    for w in text.split(\" \"):\n",
    "        #lemmaWords.append(lemmatizer.lemmatize(w, getWordNetPOSTag(w)))\n",
    "        lemmaWords.append(lemmatizer.lemmatize(w, pos='a')) \n",
    "        \n",
    "    return \" \".join(lemmaWords)\n",
    "\n",
    "def porterStemmer(text):\n",
    "    porter = PorterStemmer()\n",
    "    stemmedWords = []\n",
    "    for w in text.split(\" \"):\n",
    "        stemmedWords.append(porter.stem(w)) \n",
    "        \n",
    "    return \" \".join(stemmedWords)\n",
    "\n",
    "def snowballStemmer(text):\n",
    "    porter = SnowballStemmer(\"english\")\n",
    "    stemmedWords = []\n",
    "    for w in text.split(\" \"):\n",
    "        stemmedWords.append(porter.stem(w)) \n",
    "        \n",
    "    return \" \".join(stemmedWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FEATURES\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "def crear_features(df):\n",
    "    \n",
    "    #texto\n",
    "    df['clean_text'] = df['text'].apply(clean)\n",
    "    df['lemma_text'] = df['clean_text'].apply(lemmatizer)\n",
    "    df['porter_stemmed_text'] = df['clean_text'].apply(porterStemmer)\n",
    "    df['snowball_stemmed_text'] = df['clean_text'].apply(snowballStemmer)\n",
    "    \n",
    "    #categóricas\n",
    "    df['words'] = df['text'].apply(lambda x: x.split(' '))\n",
    "    df['hashtags'] = df['words'].apply(getHashtags) #Obtengo los hashtags: \"ht1 ht2 ht3 ...\"\n",
    "    df['mentions'] = df['words'].apply(getMentions) #Obtengo las menciones: \"men1 men2 men3 ...\"\n",
    "    df['urls'] = df['words'].apply(getURLs) #Obtengo las urls \"url1 url2 url3 ...\"\n",
    "    df['stop_words'] = df['text'].apply(lambda x: [w for w in str(x).lower().split() if w in stop])\n",
    "    \n",
    "    #numéricas\n",
    "    df['words_count'] = df['words'].apply(lambda x: len(x))\n",
    "    df['character_count'] = df['text'].str.len()\n",
    "    df['mean_word_length'] = df['text'].apply(lambda x: (sum(len(w) for w in str(x).split()) / len(str(x).split())))\n",
    "    df['punctuation_count'] = df['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n",
    "    df['stop_words_count'] = df['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in stop]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>lemma_text</th>\n",
       "      <th>porter_stemmed_text</th>\n",
       "      <th>snowball_stemmed_text</th>\n",
       "      <th>words</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>mentions</th>\n",
       "      <th>urls</th>\n",
       "      <th>stop_words</th>\n",
       "      <th>words_count</th>\n",
       "      <th>character_count</th>\n",
       "      <th>mean_word_length</th>\n",
       "      <th>punctuation_count</th>\n",
       "      <th>stop_words_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>Our Deeds Reason earthquake May ALLAH Forgive us</td>\n",
       "      <td>Our Deeds Reason earthquake May ALLAH Forgive us</td>\n",
       "      <td>our deed reason earthquak may allah forgiv us</td>\n",
       "      <td>our deed reason earthquak may allah forgiv us</td>\n",
       "      <td>[Our, Deeds, are, the, Reason, of, this, #eart...</td>\n",
       "      <td>earthquake</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[our, are, the, of, this, all]</td>\n",
       "      <td>13</td>\n",
       "      <td>69</td>\n",
       "      <td>4.384615</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>Forest fire near La Ronge Sask Canada</td>\n",
       "      <td>Forest fire near La Ronge Sask Canada</td>\n",
       "      <td>forest fire near La rong sask canada</td>\n",
       "      <td>forest fire near la rong sask canada</td>\n",
       "      <td>[Forest, fire, near, La, Ronge, Sask., Canada]</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>7</td>\n",
       "      <td>38</td>\n",
       "      <td>4.571429</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>All residents asked shelter place notified off...</td>\n",
       "      <td>All residents asked shelter place notified off...</td>\n",
       "      <td>all resid ask shelter place notifi offic No ev...</td>\n",
       "      <td>all resid ask shelter place notifi offic no ev...</td>\n",
       "      <td>[All, residents, asked, to, 'shelter, in, plac...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[all, to, in, are, being, by, no, other, or, i...</td>\n",
       "      <td>22</td>\n",
       "      <td>133</td>\n",
       "      <td>5.090909</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>13000 people receive wildfires evacuation orde...</td>\n",
       "      <td>13000 people receive wildfires evacuation orde...</td>\n",
       "      <td>13000 peopl receiv wildfir evacu order califor...</td>\n",
       "      <td>13000 peopl receiv wildfir evacu order califor...</td>\n",
       "      <td>[13,000, people, receive, #wildfires, evacuati...</td>\n",
       "      <td>wildfires</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[in]</td>\n",
       "      <td>9</td>\n",
       "      <td>65</td>\n",
       "      <td>7.125000</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Just got sent photo Ruby Alaska smoke wildfire...</td>\n",
       "      <td>Just got sent photo Ruby Alaska smoke wildfire...</td>\n",
       "      <td>just got sent photo rubi alaska smoke wildfir ...</td>\n",
       "      <td>just got sent photo rubi alaska smoke wildfir ...</td>\n",
       "      <td>[Just, got, sent, this, photo, from, Ruby, #Al...</td>\n",
       "      <td>Alaska wildfires</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[just, this, from, as, from, into, a]</td>\n",
       "      <td>17</td>\n",
       "      <td>88</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target                                         clean_text  \\\n",
       "0       1   Our Deeds Reason earthquake May ALLAH Forgive us   \n",
       "1       1              Forest fire near La Ronge Sask Canada   \n",
       "2       1  All residents asked shelter place notified off...   \n",
       "3       1  13000 people receive wildfires evacuation orde...   \n",
       "4       1  Just got sent photo Ruby Alaska smoke wildfire...   \n",
       "\n",
       "                                          lemma_text  \\\n",
       "0   Our Deeds Reason earthquake May ALLAH Forgive us   \n",
       "1              Forest fire near La Ronge Sask Canada   \n",
       "2  All residents asked shelter place notified off...   \n",
       "3  13000 people receive wildfires evacuation orde...   \n",
       "4  Just got sent photo Ruby Alaska smoke wildfire...   \n",
       "\n",
       "                                 porter_stemmed_text  \\\n",
       "0      our deed reason earthquak may allah forgiv us   \n",
       "1               forest fire near La rong sask canada   \n",
       "2  all resid ask shelter place notifi offic No ev...   \n",
       "3  13000 peopl receiv wildfir evacu order califor...   \n",
       "4  just got sent photo rubi alaska smoke wildfir ...   \n",
       "\n",
       "                               snowball_stemmed_text  \\\n",
       "0      our deed reason earthquak may allah forgiv us   \n",
       "1               forest fire near la rong sask canada   \n",
       "2  all resid ask shelter place notifi offic no ev...   \n",
       "3  13000 peopl receiv wildfir evacu order califor...   \n",
       "4  just got sent photo rubi alaska smoke wildfir ...   \n",
       "\n",
       "                                               words          hashtags  \\\n",
       "0  [Our, Deeds, are, the, Reason, of, this, #eart...        earthquake   \n",
       "1     [Forest, fire, near, La, Ronge, Sask., Canada]                     \n",
       "2  [All, residents, asked, to, 'shelter, in, plac...                     \n",
       "3  [13,000, people, receive, #wildfires, evacuati...         wildfires   \n",
       "4  [Just, got, sent, this, photo, from, Ruby, #Al...  Alaska wildfires   \n",
       "\n",
       "  mentions urls                                         stop_words  \\\n",
       "0                                   [our, are, the, of, this, all]   \n",
       "1                                                               []   \n",
       "2                [all, to, in, are, being, by, no, other, or, i...   \n",
       "3                                                             [in]   \n",
       "4                            [just, this, from, as, from, into, a]   \n",
       "\n",
       "   words_count  character_count  mean_word_length  punctuation_count  \\\n",
       "0           13               69          4.384615                  1   \n",
       "1            7               38          4.571429                  1   \n",
       "2           22              133          5.090909                  3   \n",
       "3            9               65          7.125000                  2   \n",
       "4           17               88          4.500000                  2   \n",
       "\n",
       "   stop_words_count  \n",
       "0                 6  \n",
       "1                 0  \n",
       "2                11  \n",
       "3                 1  \n",
       "4                 7  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crear_features(train_df)\n",
    "crear_features(test_df)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trp(l, n):\n",
    "    return l[:n] + [0]*(n-len(l))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizacion_de_texto(texto,longitud):\n",
    "    marked_text = \"[CLS] \" + texto + \" [SEP]\"\n",
    "    tokenized_text = tokenizer.tokenize(marked_text)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    vector = trp(indexed_tokens,longitud)\n",
    "    return vector\n",
    "    #return indexed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.fillna('')\n",
    "test_df = test_df.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parametros\n",
    "vocab_size = 30522\n",
    "epochs = 2\n",
    "maxlen = 72\n",
    "n_words = 500\n",
    "test_size = 0.25\n",
    "padding_texto = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenización\n",
    "def tokenizar_df(df):\n",
    "\n",
    "    df['texto_tokenizado'] = df['text'].apply(lambda x: tokenizacion_de_texto(x,padding_texto))\n",
    "    df['lemma_text_tokenizado'] = df['lemma_text'].apply(lambda x: tokenizacion_de_texto(x,padding_texto))\n",
    "    df['porter_stemmed_text_tokenizado'] = df['porter_stemmed_text'].apply(lambda x: tokenizacion_de_texto(x,padding_texto))\n",
    "    df['snowball_stemmed_text_tokenizado'] = df['snowball_stemmed_text'].apply(lambda x: tokenizacion_de_texto(x,padding_texto))\n",
    "    df['clean_text_tokenizado'] = df['clean_text'].apply(lambda x: tokenizacion_de_texto(x,padding_texto))\n",
    "    df['keyword_tokenizado'] = df['keyword'].apply(lambda x: tokenizacion_de_texto(x,5))\n",
    "    df['location_tokenizado'] = df['location'].apply(lambda x: tokenizacion_de_texto(x,7))\n",
    "\n",
    "    df['words_count'] = df['words_count'].apply(lambda x: [x])\n",
    "    df['character_count'] = df['character_count'].apply(lambda x: [x])\n",
    "    df['mean_word_length'] = df['mean_word_length'].apply(lambda x: [x])\n",
    "    df['punctuation_count'] = df['punctuation_count'].apply(lambda x: [x])\n",
    "    df['stop_words_count'] = df['stop_words_count'].apply(lambda x: [x])\n",
    "    \n",
    "    df['features_tokenizados'] = (  #TEXTO\n",
    "                                    df['texto_tokenizado']\n",
    "                                    # df['clean_text_tokenizado']\n",
    "                                    #df['lemma_text_tokenizado']\n",
    "                                    #df['porter_stemmed_text_tokenizado']\n",
    "                                    #df['snowball_stemmed_text_tokenizado']\n",
    "        \n",
    "                                    #CATEGORICAS\n",
    "                                    + df['keyword_tokenizado']\n",
    "                                    + df['location_tokenizado']\n",
    "                                  \n",
    "                                    #NUMERICAS\n",
    "                                    #+ df['mean_word_length']\n",
    "                                    #+ df['stop_words_count']\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>lemma_text</th>\n",
       "      <th>porter_stemmed_text</th>\n",
       "      <th>snowball_stemmed_text</th>\n",
       "      <th>words</th>\n",
       "      <th>...</th>\n",
       "      <th>punctuation_count</th>\n",
       "      <th>stop_words_count</th>\n",
       "      <th>texto_tokenizado</th>\n",
       "      <th>lemma_text_tokenizado</th>\n",
       "      <th>porter_stemmed_text_tokenizado</th>\n",
       "      <th>snowball_stemmed_text_tokenizado</th>\n",
       "      <th>clean_text_tokenizado</th>\n",
       "      <th>keyword_tokenizado</th>\n",
       "      <th>location_tokenizado</th>\n",
       "      <th>features_tokenizados</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>Our Deeds Reason earthquake May ALLAH Forgive us</td>\n",
       "      <td>Our Deeds Reason earthquake May ALLAH Forgive us</td>\n",
       "      <td>our deed reason earthquak may allah forgiv us</td>\n",
       "      <td>our deed reason earthquak may allah forgiv us</td>\n",
       "      <td>[Our, Deeds, are, the, Reason, of, this, #eart...</td>\n",
       "      <td>...</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[6]</td>\n",
       "      <td>[101, 2256, 15616, 2024, 1996, 3114, 1997, 202...</td>\n",
       "      <td>[101, 2256, 15616, 3114, 8372, 2089, 16455, 96...</td>\n",
       "      <td>[101, 2256, 15046, 3114, 3011, 16211, 2243, 20...</td>\n",
       "      <td>[101, 2256, 15046, 3114, 3011, 16211, 2243, 20...</td>\n",
       "      <td>[101, 2256, 15616, 3114, 8372, 2089, 16455, 96...</td>\n",
       "      <td>[101, 102, 0, 0, 0]</td>\n",
       "      <td>[101, 102, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[101, 2256, 15616, 2024, 1996, 3114, 1997, 202...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>Forest fire near La Ronge Sask Canada</td>\n",
       "      <td>Forest fire near La Ronge Sask Canada</td>\n",
       "      <td>forest fire near La rong sask canada</td>\n",
       "      <td>forest fire near la rong sask canada</td>\n",
       "      <td>[Forest, fire, near, La, Ronge, Sask., Canada]</td>\n",
       "      <td>...</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[101, 3224, 2543, 2379, 2474, 6902, 3351, 2187...</td>\n",
       "      <td>[101, 3224, 2543, 2379, 2474, 6902, 3351, 2187...</td>\n",
       "      <td>[101, 3224, 2543, 2379, 2474, 6902, 2290, 2187...</td>\n",
       "      <td>[101, 3224, 2543, 2379, 2474, 6902, 2290, 2187...</td>\n",
       "      <td>[101, 3224, 2543, 2379, 2474, 6902, 3351, 2187...</td>\n",
       "      <td>[101, 102, 0, 0, 0]</td>\n",
       "      <td>[101, 102, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[101, 3224, 2543, 2379, 2474, 6902, 3351, 2187...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>All residents asked shelter place notified off...</td>\n",
       "      <td>All residents asked shelter place notified off...</td>\n",
       "      <td>all resid ask shelter place notifi offic No ev...</td>\n",
       "      <td>all resid ask shelter place notifi offic no ev...</td>\n",
       "      <td>[All, residents, asked, to, 'shelter, in, plac...</td>\n",
       "      <td>...</td>\n",
       "      <td>[3]</td>\n",
       "      <td>[11]</td>\n",
       "      <td>[101, 2035, 3901, 2356, 2000, 1005, 7713, 1999...</td>\n",
       "      <td>[101, 2035, 3901, 2356, 7713, 2173, 19488, 373...</td>\n",
       "      <td>[101, 2035, 24501, 3593, 3198, 7713, 2173, 202...</td>\n",
       "      <td>[101, 2035, 24501, 3593, 3198, 7713, 2173, 202...</td>\n",
       "      <td>[101, 2035, 3901, 2356, 7713, 2173, 19488, 373...</td>\n",
       "      <td>[101, 102, 0, 0, 0]</td>\n",
       "      <td>[101, 102, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[101, 2035, 3901, 2356, 2000, 1005, 7713, 1999...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>13000 people receive wildfires evacuation orde...</td>\n",
       "      <td>13000 people receive wildfires evacuation orde...</td>\n",
       "      <td>13000 peopl receiv wildfir evacu order califor...</td>\n",
       "      <td>13000 peopl receiv wildfir evacu order califor...</td>\n",
       "      <td>[13,000, people, receive, #wildfires, evacuati...</td>\n",
       "      <td>...</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[101, 2410, 1010, 2199, 2111, 4374, 1001, 3748...</td>\n",
       "      <td>[101, 19527, 2692, 2111, 4374, 3748, 26332, 13...</td>\n",
       "      <td>[101, 19527, 2692, 21877, 7361, 2140, 28667, 7...</td>\n",
       "      <td>[101, 19527, 2692, 21877, 7361, 2140, 28667, 7...</td>\n",
       "      <td>[101, 19527, 2692, 2111, 4374, 3748, 26332, 13...</td>\n",
       "      <td>[101, 102, 0, 0, 0]</td>\n",
       "      <td>[101, 102, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[101, 2410, 1010, 2199, 2111, 4374, 1001, 3748...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Just got sent photo Ruby Alaska smoke wildfire...</td>\n",
       "      <td>Just got sent photo Ruby Alaska smoke wildfire...</td>\n",
       "      <td>just got sent photo rubi alaska smoke wildfir ...</td>\n",
       "      <td>just got sent photo rubi alaska smoke wildfir ...</td>\n",
       "      <td>[Just, got, sent, this, photo, from, Ruby, #Al...</td>\n",
       "      <td>...</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[7]</td>\n",
       "      <td>[101, 2074, 2288, 2741, 2023, 6302, 2013, 1009...</td>\n",
       "      <td>[101, 2074, 2288, 2741, 6302, 10090, 7397, 561...</td>\n",
       "      <td>[101, 2074, 2288, 2741, 6302, 14548, 2072, 739...</td>\n",
       "      <td>[101, 2074, 2288, 2741, 6302, 14548, 2072, 739...</td>\n",
       "      <td>[101, 2074, 2288, 2741, 6302, 10090, 7397, 561...</td>\n",
       "      <td>[101, 102, 0, 0, 0]</td>\n",
       "      <td>[101, 102, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[101, 2074, 2288, 2741, 2023, 6302, 2013, 1009...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1                   Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4                              Forest fire near La Ronge Sask. Canada   \n",
       "2   5                   All residents asked to 'shelter in place' are ...   \n",
       "3   6                   13,000 people receive #wildfires evacuation or...   \n",
       "4   7                   Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target                                         clean_text  \\\n",
       "0       1   Our Deeds Reason earthquake May ALLAH Forgive us   \n",
       "1       1              Forest fire near La Ronge Sask Canada   \n",
       "2       1  All residents asked shelter place notified off...   \n",
       "3       1  13000 people receive wildfires evacuation orde...   \n",
       "4       1  Just got sent photo Ruby Alaska smoke wildfire...   \n",
       "\n",
       "                                          lemma_text  \\\n",
       "0   Our Deeds Reason earthquake May ALLAH Forgive us   \n",
       "1              Forest fire near La Ronge Sask Canada   \n",
       "2  All residents asked shelter place notified off...   \n",
       "3  13000 people receive wildfires evacuation orde...   \n",
       "4  Just got sent photo Ruby Alaska smoke wildfire...   \n",
       "\n",
       "                                 porter_stemmed_text  \\\n",
       "0      our deed reason earthquak may allah forgiv us   \n",
       "1               forest fire near La rong sask canada   \n",
       "2  all resid ask shelter place notifi offic No ev...   \n",
       "3  13000 peopl receiv wildfir evacu order califor...   \n",
       "4  just got sent photo rubi alaska smoke wildfir ...   \n",
       "\n",
       "                               snowball_stemmed_text  \\\n",
       "0      our deed reason earthquak may allah forgiv us   \n",
       "1               forest fire near la rong sask canada   \n",
       "2  all resid ask shelter place notifi offic no ev...   \n",
       "3  13000 peopl receiv wildfir evacu order califor...   \n",
       "4  just got sent photo rubi alaska smoke wildfir ...   \n",
       "\n",
       "                                               words  ... punctuation_count  \\\n",
       "0  [Our, Deeds, are, the, Reason, of, this, #eart...  ...               [1]   \n",
       "1     [Forest, fire, near, La, Ronge, Sask., Canada]  ...               [1]   \n",
       "2  [All, residents, asked, to, 'shelter, in, plac...  ...               [3]   \n",
       "3  [13,000, people, receive, #wildfires, evacuati...  ...               [2]   \n",
       "4  [Just, got, sent, this, photo, from, Ruby, #Al...  ...               [2]   \n",
       "\n",
       "  stop_words_count                                   texto_tokenizado  \\\n",
       "0              [6]  [101, 2256, 15616, 2024, 1996, 3114, 1997, 202...   \n",
       "1              [0]  [101, 3224, 2543, 2379, 2474, 6902, 3351, 2187...   \n",
       "2             [11]  [101, 2035, 3901, 2356, 2000, 1005, 7713, 1999...   \n",
       "3              [1]  [101, 2410, 1010, 2199, 2111, 4374, 1001, 3748...   \n",
       "4              [7]  [101, 2074, 2288, 2741, 2023, 6302, 2013, 1009...   \n",
       "\n",
       "                               lemma_text_tokenizado  \\\n",
       "0  [101, 2256, 15616, 3114, 8372, 2089, 16455, 96...   \n",
       "1  [101, 3224, 2543, 2379, 2474, 6902, 3351, 2187...   \n",
       "2  [101, 2035, 3901, 2356, 7713, 2173, 19488, 373...   \n",
       "3  [101, 19527, 2692, 2111, 4374, 3748, 26332, 13...   \n",
       "4  [101, 2074, 2288, 2741, 6302, 10090, 7397, 561...   \n",
       "\n",
       "                      porter_stemmed_text_tokenizado  \\\n",
       "0  [101, 2256, 15046, 3114, 3011, 16211, 2243, 20...   \n",
       "1  [101, 3224, 2543, 2379, 2474, 6902, 2290, 2187...   \n",
       "2  [101, 2035, 24501, 3593, 3198, 7713, 2173, 202...   \n",
       "3  [101, 19527, 2692, 21877, 7361, 2140, 28667, 7...   \n",
       "4  [101, 2074, 2288, 2741, 6302, 14548, 2072, 739...   \n",
       "\n",
       "                    snowball_stemmed_text_tokenizado  \\\n",
       "0  [101, 2256, 15046, 3114, 3011, 16211, 2243, 20...   \n",
       "1  [101, 3224, 2543, 2379, 2474, 6902, 2290, 2187...   \n",
       "2  [101, 2035, 24501, 3593, 3198, 7713, 2173, 202...   \n",
       "3  [101, 19527, 2692, 21877, 7361, 2140, 28667, 7...   \n",
       "4  [101, 2074, 2288, 2741, 6302, 14548, 2072, 739...   \n",
       "\n",
       "                               clean_text_tokenizado   keyword_tokenizado  \\\n",
       "0  [101, 2256, 15616, 3114, 8372, 2089, 16455, 96...  [101, 102, 0, 0, 0]   \n",
       "1  [101, 3224, 2543, 2379, 2474, 6902, 3351, 2187...  [101, 102, 0, 0, 0]   \n",
       "2  [101, 2035, 3901, 2356, 7713, 2173, 19488, 373...  [101, 102, 0, 0, 0]   \n",
       "3  [101, 19527, 2692, 2111, 4374, 3748, 26332, 13...  [101, 102, 0, 0, 0]   \n",
       "4  [101, 2074, 2288, 2741, 6302, 10090, 7397, 561...  [101, 102, 0, 0, 0]   \n",
       "\n",
       "         location_tokenizado  \\\n",
       "0  [101, 102, 0, 0, 0, 0, 0]   \n",
       "1  [101, 102, 0, 0, 0, 0, 0]   \n",
       "2  [101, 102, 0, 0, 0, 0, 0]   \n",
       "3  [101, 102, 0, 0, 0, 0, 0]   \n",
       "4  [101, 102, 0, 0, 0, 0, 0]   \n",
       "\n",
       "                                features_tokenizados  \n",
       "0  [101, 2256, 15616, 2024, 1996, 3114, 1997, 202...  \n",
       "1  [101, 3224, 2543, 2379, 2474, 6902, 3351, 2187...  \n",
       "2  [101, 2035, 3901, 2356, 2000, 1005, 7713, 1999...  \n",
       "3  [101, 2410, 1010, 2199, 2111, 4374, 1001, 3748...  \n",
       "4  [101, 2074, 2288, 2741, 2023, 6302, 2013, 1009...  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizar_df(train_df)\n",
    "tokenizar_df(test_df)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_df['features_tokenizados'], train_df['target'], test_size=test_size,\n",
    "                                                  random_state=42)\n",
    "FTest = test_df['features_tokenizados']\n",
    "test_ids = test_df['id'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def tokenizar(textos):\n",
    "    secuencia = []\n",
    "    for text in textos:\n",
    "        marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "        tokenized_text = tokenizer.tokenize(marked_text)\n",
    "        indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "        secuencia.appFailed to convert a NumPy array to a Tensor (Unsupported object type list).end(indexed_tokens)\n",
    "    return secuencia\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array([np.array(lista) for lista in X_train])\n",
    "X_a = np.array([np.array(lista) for lista in X_test])\n",
    "padded_FTest = X_test = np.array([np.array(lista) for lista in FTest])\n",
    "padded_train = pad_sequences(X_train, maxlen = maxlen, truncating = 'post')\n",
    "padded_test = pad_sequences(X_a, maxlen = maxlen, truncating = 'post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#secuencia_train = tokenizar(X_train.values)\n",
    "#secuencia_test = tokenizar(X_test.values)\n",
    "#secuencia_FTest = tokenizar(FTest.values)\n",
    "\n",
    "#padded_train = pad_sequences(secuencia_train, maxlen = maxlen, truncating = 'post')\n",
    "#padded_test = pad_sequences(secuencia_test, maxlen = maxlen)\n",
    "#padded_FTest = pad_sequences(secuencia_FTest, maxlen = maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_18 (Embedding)     (None, 72, 128)           3906816   \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 72, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 70, 256)           98560     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_11 (Glo (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "dropout_29 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 4,013,633\n",
      "Trainable params: 4,013,633\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#modelo Conv1D\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, 128, input_length= maxlen),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Conv1D(256, 3, activation='relu'),\n",
    "    tf.keras.layers.GlobalMaxPooling1D(),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5709 samples, validate on 1904 samples\n",
      "Epoch 1/2\n",
      "5709/5709 [==============================] - 13s 2ms/sample - loss: 0.6384 - accuracy: 0.6367 - val_loss: 0.5676 - val_accuracy: 0.7285\n",
      "Epoch 2/2\n",
      "5709/5709 [==============================] - 13s 2ms/sample - loss: 0.4754 - accuracy: 0.7905 - val_loss: 0.4491 - val_accuracy: 0.8030\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f1a74a1f6d0>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(padded_train, y_train, epochs=2, validation_data=(padded_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_locales = model.predict_classes(padded_test)\n",
    "preds_locales = pd.Series(list((x[0] for x in preds_locales)))\n",
    "#preds_locales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.7553816046966734\n"
     ]
    }
   ],
   "source": [
    "print(\"F1 score:\", f1_score(y_test, preds_locales))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    target\n",
       "id        \n",
       "0        1\n",
       "2        1\n",
       "3        1\n",
       "9        1\n",
       "11       1"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model.predict_classes(padded_FTest)\n",
    "#preds = pd.Series(preds)\n",
    "preds = pd.Series(list((x[0] for x in preds)))\n",
    "preds\n",
    "df_preds = pd.concat([test_ids,preds],axis=1)\n",
    "df_preds.rename(columns = {0 : 'target'}, inplace=True)\n",
    "df_preds.set_index('id', inplace=True)\n",
    "df_preds.to_csv('BERT-Conv1D-features.csv')\n",
    "df_preds.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
